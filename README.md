# theory-learning-bandits
Q-learning and game-theoretic analysis of short-horizon competitive bandits. Implements closed-form equilibrium and self-play Double Q-learning for exclusivity and discovery-rate comparative statics.

This repository contains my bachelor’s thesis, completed under the supervision of Professor Christoph Carnehl, which extends his model “When to Think and When to Do” by introducing a second, competing agent into a short, discrete-time environment. The project examines how two decision-makers balance the choice between acquiring information and exploiting what they already know when exclusive access to future rewards is at stake. It combines an analytic equilibrium derivation for short time horizons with a reinforcement-learning simulation that mirrors the theoretical setup one-for-one.

The starting point is a minimal two-player bandit game played over two or three periods. Each player simultaneously decides whether to “Do” (exploit) or to “Think” (pay a cost to learn which action is better). If exactly one player decides to Think, that player gains exclusive access to the next period’s prize. If both Think, they share that right; if neither Thinks, the prize remains contested. This simple setup captures the trade-off between learning and doing in environments where information can grant a temporary monopoly advantage.

The thesis has three main parts. First, it derives the mixed-strategy Nash equilibrium for the two-period game, showing how the probability of choosing to Think rises with the likelihood of a successful discovery and falls with the cost of thinking. Second, it extends the model to a three-period version in which exclusivity lasts for two future periods. This strengthens incentives to Think and shifts the equilibrium toward more frequent information acquisition. Third, it implements a self-play Double Q-learning simulation in Python to see whether reinforcement-learning agents independently rediscover the same behavioral patterns predicted by theory.

The simulation results reproduce the main comparative-statics predictions: agents Think more often when discoveries are easier or more valuable, and less when thinking is expensive. Extending exclusivity into additional future periods reliably increases the share of thinkers. Quantitatively, the learned frequencies tend to fall slightly below the theoretical benchmark, especially when discoveries are rare or rewards are delayed, which is consistent with known learning frictions in non-stationary two-player settings.

All experiments are reproducible using standard Python libraries. The repository includes Jupyter notebooks for generating the analytical figures, running the self-play training loops, and producing the plots used in the thesis. The code was kept deliberately minimal to make the economic mechanisms transparent rather than to optimize performance.

The broader takeaway is that exclusivity acts as a powerful incentive for information acquisition: when future access depends on learning first, even small increases in discovery likelihood can trigger a race to think. Conversely, when discoveries are rare or can be postponed, agents default to exploitation. By combining analytic and computational methods, the project shows how simple learning dynamics can approximate equilibrium reasoning in strategic environments with time pressure and exclusivity.
